{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython candies...\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports we need.\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq with PyTorch\n",
    "====\n",
    "\n",
    "Sequence-to-Sequence (Seq2Seq) learning is a useful class of neural network model to map sequential input into an output sequence. It has been shown to work well on various task, from machine translation to interpreting Python without an interpreter. {{citations-needed}}\n",
    "\n",
    "This notebook is a hands-on session to write an encoder-decoder Seq2Seq network using PyTorch for [DataScience SG meetup](https://www.meetup.com/DataScience-SG-Singapore/events/246541733/). \n",
    "\n",
    "\n",
    "It would be great if you have at least worked through the [\"Deep Learning in 60 minutes\" PyTorch tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) before continuing the rest of the notebook.\n",
    "\n",
    "\n",
    "Acknowledgements\n",
    "----\n",
    "The materials are largely based on the \n",
    "\n",
    " - [intermediate PyTorch tutorials by Sean Robertson](http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) and \n",
    " - [Luong et al. tutorial on neural machine translation in ACL16](https://sites.google.com/site/acl16nmt/home).\n",
    "\n",
    "The dataset used in this exercise is hosted on https://www.kaggle.com/alvations/sg-kopi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopi Problems\n",
    "====\n",
    "\n",
    "In this hands-on session, we want to **train a neural network to translate from Singlish Kopi orders to English?**\n",
    "\n",
    "\n",
    "**\"Singlish\" -> English**\n",
    "\n",
    "```\n",
    "\"Kopi\" -> Coffee with condensed milk\n",
    "\"Kopi O\" -> Coffee without milk or sugar\n",
    "\"Kopi dinosaur gau siew dai peng\" -> ???\n",
    "```\n",
    "\n",
    "(Image Source: http://www.straitstimes.com/lifestyle/food/get-your-kopi-kick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://static.straitstimes.com.sg/sites/default/files/160522_kopi.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://static.straitstimes.com.sg/sites/default/files/160522_kopi.jpg\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriously?\n",
    "----\n",
    "\n",
    "Yes, we'll be translating Singlish Kopi orders to English using the [sequence-to-sequence network](https://arxiv.org/abs/1409.3215) {{citations-needed}}. \n",
    "\n",
    "But first...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Munging\n",
    "====\n",
    "\n",
    "Before any machine/deep learning, we have to get some data and \"hammer\" it until we get it into the shape we want.\n",
    "\n",
    "> *Data scientists spend 60% of their time on cleaning and organizing data. Collecting data sets comes second at 19% of their time, meaning data scientists spend around 80% of their time on preparing and managing data for analysis.*\n",
    "\n",
    "> (Source: [Gil Press](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#3e4dc0416f63) Forbes article)\n",
    "\n",
    "**Step 1:** Take the data from somewhere, in this case: http://kaggle.com/alvations/sg-kopi.\n",
    "\n",
    "**Step 2:** Import your favorite dataframe and text processing library.\n",
    "\n",
    "**Step 3:** Munge the data till desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Local Terms</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kopi O</td>\n",
       "      <td>Black Coffee with Sugar</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kopi</td>\n",
       "      <td>Black Coffee with Condensed Milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kopi C</td>\n",
       "      <td>Black Coffee with Evaporated Milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kopi Kosong</td>\n",
       "      <td>Black Coffee without sugar or milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kopi Gah Dai</td>\n",
       "      <td>Black Coffee with extra condensed milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Local Terms                                 Meaning  \\\n",
       "0        Kopi O                 Black Coffee with Sugar   \n",
       "1          Kopi        Black Coffee with Condensed Milk   \n",
       "2        Kopi C       Black Coffee with Evaporated Milk   \n",
       "3   Kopi Kosong      Black Coffee without sugar or milk   \n",
       "4  Kopi Gah Dai  Black Coffee with extra condensed milk   \n",
       "\n",
       "                                              Source  \n",
       "0  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "1  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "2  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "3  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "4  https://daneshd.com/2010/02/28/a-rough-guide-t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Reads the tab-delimited data using Pandas.\n",
    "kopitiam = pd.read_csv('coffee-culture-sg.tsv', sep='\\t')\n",
    "kopitiam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence: ['<s>', 'kopi', 'o', '</s>']\n",
      "First English sentence: ['<s>', 'black', 'coffee', 'with', 'sugar', '</s>']\n"
     ]
    }
   ],
   "source": [
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "singlish_sents = [START] + kopitiam['Local Terms'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "english_sents = [START] + kopitiam['Meaning'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First Singlish sentence:', singlish_sents[0])\n",
    "print('First English sentence:', english_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Singlish words:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, 'kopi'), (4, 'o'), (5, 'c'), (6, 'kosong'), (7, 'dai'), (8, 'gah'), (9, 'siew')]\n",
      "\n",
      "First 10 English words:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, 'black'), (4, 'coffee'), (5, 'sugar'), (6, 'with'), (7, 'condensed'), (8, 'milk'), (9, 'evaporated')]\n"
     ]
    }
   ],
   "source": [
    "# Let's convert the individual words into some sort of unique index \n",
    "# and use the unique to represent the words. \n",
    "## Cut-away: Integers = 1-2 bytes vs UTF-8 Strings = no. of chars * 1-2 bytes. @_@\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "singlish_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "singlish_vocab.add_documents(singlish_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Singlish words:\\n', sorted(singlish_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'black', 'coffee', 'with', 'sugar', '</s>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence: [0, 3, 4, 1]\n",
      "First English sentence: [0, 3, 4, 6, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "# Now, convert all the sentences into list of the indices \n",
    "print('First Singlish sentence:', singlish_vocab.doc2idx(singlish_sents[0]) )\n",
    "print('First English sentence:', english_vocab.doc2idx(english_sents[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 22, 11, 9, 7, 12, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets create a function to convert new sentences into the indexed forms.\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END])\n",
    "\n",
    "new_kopi = \"Kopi dinosaur gau siew dai peng\"\n",
    "vectorize_sent(new_kopi, singlish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0\n",
       "    3\n",
       "   22\n",
       "   11\n",
       "    9\n",
       "    7\n",
       "   12\n",
       "    1\n",
       "[torch.cuda.LongTensor of size 8x1 (GPU 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the last step of data hammering, we need to clobber \n",
    "# the vectorized sentence into PyTorch Variable. \n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "new_kopi = \"Kopi dinosaur gau siew dai peng\"\n",
    "variable_from_sent(new_kopi, singlish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the sentence length.\n",
    "variable_from_sent(new_kopi, singlish_vocab).size()[0] # Includes START and END symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the whole training corpus.\n",
    "singlish_tensors = kopitiam['Local Terms'].apply(lambda s: variable_from_sent(s, singlish_vocab))\n",
    "english_tensors = kopitiam['Meaning'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "\n",
    "sent_pairs = list(zip(singlish_tensors, english_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The Seq2Seq Model\n",
    "====\n",
    "\n",
    "A Recurrent Neural Network (RNN), is a network that operates on a sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "> *The general idea is to make **two recurrent neural network transform from one sequence to another**. An encoder network condenses an input sequence into a vector and a decoder netwrok unfolds the vector into a new sequence.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. The Encoder\n",
    "====\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
    "\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/encoder-network.png\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Simple Decoder\n",
    "====\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string `<s>` token, and the first hidden state is the context vector (the encoder’s last hidden state).\n",
    "\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/decoder-network.png\" align='left'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of output (i.e. no. of words in output vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Set the output layer to output a specific symbol \n",
    "        # from the output vocabulary\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # Transform the embedded output with a relu function. \n",
    "        output = F.relu(output)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Take the updated output and find the most appropriate\n",
    "        # output symbol. \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Training the Model\n",
    "====\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the `<s>` token as its first input, and the last hidden state of the encoder as its first hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 Set the Hyperparamters and Prepare Data (again...)\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "learning_rate=0.01\n",
    "batch_size = 2\n",
    "epochs = 30 # Since we are taking batch_size=2 and epochs=30, we only look at 600 data points.\n",
    "criterion = nn.NLLLoss()\n",
    "MAX_LENGTH=20\n",
    "\n",
    "# Initialize the network for encoder and decoder.\n",
    "input_vocab, output_vocab = singlish_vocab, english_vocab\n",
    "encoder = EncoderRNN(len(input_vocab), hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, len(output_vocab))\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "# Initialize the optimizer for encoder and decoder.\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# If batchsize == 1, choose 1 data points per batch:\n",
    "##training_data = [[random.choice(sent_pairs)] for i in range(epochs)]\n",
    "# If batch_size > 1, use random.sample() instead of random.choice:\n",
    "training_data = [random.sample(sent_pairs, batch_size) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2. Loop through the batches\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3. Iterating through each word in the encoder.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) for the GRU.\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3.1. Outputs of the Encoder\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(68, 10)\n",
      "  (gru): GRU(10, 10)\n",
      ") \n",
      "\n",
      "Dictionary(68 unique tokens: ['tut', 'jia', 'peng', 'michael', 'hao']...)\n",
      "\n",
      "########\n",
      "\n",
      "Variable containing:\n",
      "  0\n",
      "  3\n",
      " 36\n",
      "  1\n",
      "[torch.cuda.LongTensor of size 4x1 (GPU 0)]\n",
      "\n",
      "########\n",
      "\n",
      "[0, 3, 36, 1] \n",
      "\n",
      "########\n",
      "\n",
      "<s> kopi poh </s>\n",
      "\n",
      "########\n",
      "\n",
      "Variable containing:\n",
      "-0.0972  0.2290  0.1250  0.4694  0.1017 -0.2808 -0.2127  0.2121 -0.3759  0.0586\n",
      " 0.0574 -0.0617 -0.0349  0.0509 -0.0723 -0.3486 -0.1432  0.2439 -0.6072 -0.4710\n",
      " 0.5366 -0.1668 -0.1907 -0.3316 -0.3769  0.1730 -0.3404  0.4280 -0.6774 -0.4104\n",
      " 0.2478  0.0491  0.2956  0.0742 -0.0361  0.0736 -0.4051  0.2031 -0.4853 -0.3444\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 20x10 (GPU 0)]\n",
      "\n",
      "\n",
      "########\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.2478  0.0491  0.2956  0.0742 -0.0361  0.0736 -0.4051  0.2031 -0.4853\n",
      "\n",
      "Columns 9 to 9 \n",
      "  -0.3444\n",
      "[torch.cuda.FloatTensor of size 1x1x10 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cut-away: The encoded output for the last sentence in out training_data\"\n",
    "\n",
    "# The encoder has 68 unique words\n",
    "print(encoder, '\\n')\n",
    "print(singlish_vocab)\n",
    "print('\\n########\\n')\n",
    "\n",
    "# The last input sentence, in PyTorch Tensor data structure.\n",
    "print(data_batch[-1][0]) \n",
    "print('########\\n')\n",
    "\n",
    "# The last input sentence as list(int)\n",
    "print(list(map(int, data_batch[-1][0])), '\\n')\n",
    "print('########\\n')\n",
    "\n",
    "# The last input sentence as list(int)\n",
    "print(' '.join([singlish_vocab[i] for i in map(int, data_batch[-1][0])]))\n",
    "print('\\n########\\n')\n",
    "\n",
    "# The encoded outputs of the last sentence \n",
    "# Note: We have a matrix of 20 (MAX_LENGTH) x 10 (hidden_size) and \n",
    "#       for this particular sentence, we only have 4 encoded outputs\n",
    "print(encoder_outputs)\n",
    "\n",
    "print('\\n########\\n')\n",
    "\n",
    "# The last hidden state of the last input sentence. \n",
    "# Note: For vanilla RNN (Elman Net), the last hidden state of the encoder\n",
    "#       is the start state of the decoder's hidden state.\n",
    "print(encoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4. Iterating through each word in the decoder.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        \n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) -_-|||\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n",
    "            \n",
    "            #############################################\n",
    "            # 2.3.4.  Iterating through each word in the decoder.\n",
    "            #############################################\n",
    "            # Initialize the variable input with the index of the START.\n",
    "            decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            # As the first state of the decoder, we take the last step of the encoder.\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Iterate through each state in the decoder.\n",
    "            # Note: when we are training we know the length of the decoder.\n",
    "            #       so we can use the trick to restrict the loop when decoding.\n",
    "            for di in range(target_length):\n",
    "                # We move forward through each state.\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # What are alll these weird syntax, refer to 2.3.4.1\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                # Replace our decoder input for the next state with the\n",
    "                # embedding of the decoded topi guess. \n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                # Update our loss for this batch.\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                \n",
    "                # If we see the </s> symbol, break the training.\n",
    "                if ni == END_IDX:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4.1 Outputs of the Decoder\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(117, 10)\n",
      "  (gru): GRU(10, 10)\n",
      "  (softmax): LogSoftmax()\n",
      "  (out): Linear(in_features=10, out_features=117)\n",
      ") \n",
      "\n",
      "Dictionary(117 unique tokens: ['stronger', 'syrup', 'horlicks', 'brew', 'ginger']...)\n",
      "\n",
      "########\n",
      "\n",
      "<s> kopi poh </s>\n",
      "<s> thinner coffee more water is added to dilute the beverage . </s>\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cut-away: The decoded output for the last sentence in out training_data\"\n",
    "\n",
    "# The encoder has 117 unique words\n",
    "print(decoder, '\\n')\n",
    "print(english_vocab)\n",
    "print('\\n########\\n')\n",
    "\n",
    "# The last input sentence.\n",
    "print(' '.join([singlish_vocab[i] for i in map(int, data_batch[-1][0])]))\n",
    "# The last target sentence.\n",
    "print(' '.join([english_vocab[i] for i in map(int, data_batch[-1][1])]))\n",
    "\n",
    "print('\\n########\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "\n",
      "########\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-4.9207 -5.1512 -5.2086 -4.9887 -4.7247 -4.9769 -5.0857 -4.3333 -4.1256 -4.3008\n",
      "\n",
      "Columns 10 to 19 \n",
      "-4.4075 -4.9221 -5.0280 -4.8629 -5.2383 -5.0389 -4.5376 -4.4449 -5.0103 -4.7008\n",
      "\n",
      "Columns 20 to 29 \n",
      "-4.9679 -5.2594 -4.8193 -4.6872 -4.4805 -4.4097 -4.7942 -4.9296 -4.7486 -4.8726\n",
      "\n",
      "Columns 30 to 39 \n",
      "-4.4286 -4.8542 -5.1222 -4.6833 -5.0283 -4.7359 -4.5719 -4.8810 -4.9007 -4.5465\n",
      "\n",
      "Columns 40 to 49 \n",
      "-5.0500 -5.1666 -4.8831 -4.6379 -4.4574 -4.7411 -4.8628 -5.0775 -4.7726 -4.8609\n",
      "\n",
      "Columns 50 to 59 \n",
      "-4.9800 -4.9194 -5.0201 -4.7333 -4.4561 -4.6480 -4.8651 -4.6748 -4.8653 -4.7025\n",
      "\n",
      "Columns 60 to 69 \n",
      "-4.5608 -4.5032 -5.1159 -5.1541 -4.7839 -4.8625 -4.5954 -4.8888 -4.5934 -4.8091\n",
      "\n",
      "Columns 70 to 79 \n",
      "-4.7013 -4.6661 -4.7685 -4.6622 -4.7927 -4.9732 -5.2363 -4.3163 -5.2381 -4.8469\n",
      "\n",
      "Columns 80 to 89 \n",
      "-4.5284 -4.3267 -5.2866 -4.8497 -4.7198 -5.0783 -4.4740 -4.3546 -4.6759 -4.8425\n",
      "\n",
      "Columns 90 to 99 \n",
      "-4.2371 -5.0061 -4.6096 -5.2386 -4.5358 -4.4367 -4.7863 -4.9175 -4.4395 -4.8550\n",
      "\n",
      "Columns 100 to 109 \n",
      "-4.9949 -4.7557 -4.8157 -4.8613 -4.7201 -4.8602 -4.9675 -4.7158 -4.7883 -5.2722\n",
      "\n",
      "Columns 110 to 116 \n",
      "-4.8452 -4.7998 -4.7920 -4.4672 -5.0881 -4.6793 -5.1832\n",
      "[torch.cuda.FloatTensor of size 1x117 (GPU 0)]\n",
      "\n",
      "\n",
      "########\n",
      "(\n",
      "-4.1256\n",
      "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 8\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ")\n",
      "\n",
      "########\n",
      "\n",
      "-4.1256\n",
      "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 8\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "milk\n"
     ]
    }
   ],
   "source": [
    "# The last word in the last sentence. \n",
    "print([english_vocab[i] for i in map(int, data_batch[-1][1])][-1])\n",
    "\n",
    "print('\\n########')\n",
    "\n",
    "# The -log probability of the word that's most probably the \n",
    "# correct target word as we moved from the encoder to the decoder.\n",
    "print(decoder_output.data)\n",
    "\n",
    "print('\\n########')\n",
    "\n",
    "# The word with the highest probability\n",
    "print(decoder_output.data.topk(1))\n",
    "print('\\n########')\n",
    "\n",
    "# Take a look at what's the decoder's guess for the final word in the last sentence.\n",
    "topv, topi = decoder_output.data.topk(1)\n",
    "print(topv) # The -log probability of the decoder's guess.\n",
    "print(topi) # The index of the word in the english_vocab.\n",
    "print(english_vocab[int(topi)]) # Decoder's guess of the final word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.5 Backpropagate the Loss and Optimizers Takes a Step.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        \n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) -_-|||\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n",
    "            \n",
    "            #############################################\n",
    "            # 2.3.4.  Iterating through each word in the decoder.\n",
    "            #############################################\n",
    "            # Initialize the variable input with the index of the START.\n",
    "            decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            # As the first state of the decoder, we take the last step of the encoder.\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Iterate through each state in the decoder.\n",
    "            # Note: when we are training we know the length of the decoder.\n",
    "            #       so we can use the trick to restrict the loop when decoding.\n",
    "            for di in range(target_length):\n",
    "                # We move forward through each state.\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # What are alll these weird syntax, refer to 2.3.4.1\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                # Replace our decoder input for the next state with the\n",
    "                # embedding of the decoded topi guess. \n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                # Update our loss for this batch.\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                \n",
    "                # If we see the </s> symbol, break the training.\n",
    "                if ni == END_IDX:\n",
    "                    break\n",
    "    #####################################################\n",
    "    # 2.3.5 Backpropagate the Loss and Optimizers Takes a Step.\n",
    "    #####################################################\n",
    "    loss.backward() # Backpropagate.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Formalize the training per epoch as a function\n",
    "====\n",
    "\n",
    "Largely, the following code is from http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(input_variable, target_variable, encoder, decoder, \n",
    "                    encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to put the variables, decoder and optimizers to train per epoch.\n",
    "    \"\"\"\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "\n",
    "    # (Re-)Initialize the optimizers, clear all gradients. \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize the length of the PyTorch variables.\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # Iterating through each word in the input.\n",
    "    for ei in range(input_length):\n",
    "        # We move forward through each state.\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        # And we save the encoder outputs. \n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    # Initialize the variable input with the index of the START.\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    # As the first state of the decoder, we take the last step of the encoder.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if ni == END_IDX:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. Some Logging and Plotting Candies to Monitor Training\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6. Formalize the full training process as a function\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    training_pairs = [random.choice(sent_pairs) for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train_one_epoch(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Train\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 41m 15s) (100 0%) 2.5812\n",
      "0m 4s (- 33m 58s) (200 0%) 2.0632\n",
      "0m 5s (- 31m 7s) (300 0%) 2.0789\n",
      "0m 7s (- 29m 37s) (400 0%) 2.0274\n",
      "0m 8s (- 28m 53s) (500 0%) 2.0474\n",
      "0m 10s (- 28m 14s) (600 0%) 1.9938\n",
      "0m 11s (- 28m 3s) (700 0%) 2.0145\n",
      "0m 13s (- 27m 34s) (800 0%) 1.9128\n",
      "0m 15s (- 27m 40s) (900 0%) 1.9695\n",
      "0m 17s (- 29m 21s) (1000 1%) 1.9863\n",
      "0m 19s (- 29m 21s) (1100 1%) 1.7917\n",
      "0m 21s (- 28m 56s) (1200 1%) 1.7981\n",
      "0m 22s (- 28m 37s) (1300 1%) 1.7196\n",
      "0m 24s (- 28m 18s) (1400 1%) 1.5752\n",
      "0m 25s (- 28m 5s) (1500 1%) 1.6152\n",
      "0m 27s (- 27m 58s) (1600 1%) 1.5605\n",
      "0m 29s (- 28m 18s) (1700 1%) 1.5186\n",
      "0m 31s (- 28m 39s) (1800 1%) 1.5110\n",
      "0m 34s (- 29m 27s) (1900 1%) 1.5140\n",
      "0m 36s (- 29m 39s) (2000 2%) 1.4677\n",
      "0m 38s (- 29m 38s) (2100 2%) 1.4021\n",
      "0m 39s (- 29m 35s) (2200 2%) 1.4298\n",
      "0m 42s (- 29m 57s) (2300 2%) 1.5225\n",
      "0m 46s (- 31m 16s) (2400 2%) 1.3753\n",
      "0m 49s (- 32m 17s) (2500 2%) 1.3200\n",
      "0m 53s (- 33m 18s) (2600 2%) 1.4220\n",
      "0m 56s (- 34m 9s) (2700 2%) 1.2380\n",
      "1m 0s (- 35m 7s) (2800 2%) 1.3245\n",
      "1m 4s (- 35m 49s) (2900 2%) 1.2633\n",
      "1m 7s (- 36m 27s) (3000 3%) 1.1735\n",
      "1m 11s (- 37m 10s) (3100 3%) 1.1843\n",
      "1m 15s (- 37m 56s) (3200 3%) 1.2093\n",
      "1m 18s (- 38m 25s) (3300 3%) 1.1004\n",
      "1m 22s (- 38m 58s) (3400 3%) 1.1787\n",
      "1m 25s (- 39m 21s) (3500 3%) 1.2175\n",
      "1m 28s (- 39m 40s) (3600 3%) 1.1100\n",
      "1m 31s (- 39m 37s) (3700 3%) 1.1049\n",
      "1m 33s (- 39m 35s) (3800 3%) 0.9921\n",
      "1m 36s (- 39m 47s) (3900 3%) 1.2082\n",
      "1m 39s (- 39m 56s) (4000 4%) 1.1116\n",
      "1m 42s (- 40m 6s) (4100 4%) 1.0656\n",
      "1m 45s (- 40m 9s) (4200 4%) 1.0777\n",
      "1m 48s (- 40m 19s) (4300 4%) 1.0526\n",
      "1m 51s (- 40m 27s) (4400 4%) 1.2087\n",
      "1m 54s (- 40m 39s) (4500 4%) 1.0543\n",
      "1m 57s (- 40m 42s) (4600 4%) 0.8630\n",
      "2m 0s (- 40m 47s) (4700 4%) 0.9300\n",
      "2m 2s (- 40m 30s) (4800 4%) 0.8907\n",
      "2m 4s (- 40m 16s) (4900 4%) 1.1276\n",
      "2m 6s (- 40m 3s) (5000 5%) 0.9086\n",
      "2m 8s (- 39m 54s) (5100 5%) 0.9277\n",
      "2m 10s (- 39m 43s) (5200 5%) 0.9861\n",
      "2m 13s (- 39m 36s) (5300 5%) 0.8962\n",
      "2m 15s (- 39m 25s) (5400 5%) 0.9324\n",
      "2m 16s (- 39m 13s) (5500 5%) 0.9162\n",
      "2m 19s (- 39m 3s) (5600 5%) 0.7368\n",
      "2m 20s (- 38m 52s) (5700 5%) 0.8047\n",
      "2m 23s (- 38m 56s) (5800 5%) 0.7864\n",
      "2m 26s (- 38m 53s) (5900 5%) 0.8261\n",
      "2m 29s (- 39m 4s) (6000 6%) 0.8423\n",
      "2m 33s (- 39m 16s) (6100 6%) 0.7688\n",
      "2m 37s (- 39m 40s) (6200 6%) 0.7344\n",
      "2m 41s (- 39m 55s) (6300 6%) 0.7950\n",
      "2m 45s (- 40m 13s) (6400 6%) 0.8026\n",
      "2m 48s (- 40m 29s) (6500 6%) 0.7690\n",
      "2m 52s (- 40m 46s) (6600 6%) 0.8676\n",
      "2m 56s (- 41m 1s) (6700 6%) 0.6698\n",
      "3m 0s (- 41m 19s) (6800 6%) 0.7358\n",
      "3m 4s (- 41m 36s) (6900 6%) 0.8130\n",
      "3m 8s (- 41m 50s) (7000 7%) 0.7103\n",
      "3m 13s (- 42m 5s) (7100 7%) 0.7201\n",
      "3m 17s (- 42m 19s) (7200 7%) 0.6897\n",
      "3m 21s (- 42m 33s) (7300 7%) 0.7049\n",
      "3m 25s (- 42m 46s) (7400 7%) 0.6287\n",
      "3m 28s (- 42m 57s) (7500 7%) 0.6498\n",
      "3m 33s (- 43m 10s) (7600 7%) 0.6917\n",
      "3m 37s (- 43m 23s) (7700 7%) 0.6758\n",
      "3m 41s (- 43m 37s) (7800 7%) 0.6728\n",
      "3m 45s (- 43m 46s) (7900 7%) 0.6211\n",
      "3m 49s (- 44m 1s) (8000 8%) 0.5991\n",
      "3m 53s (- 44m 8s) (8100 8%) 0.6801\n",
      "3m 57s (- 44m 15s) (8200 8%) 0.6484\n",
      "4m 1s (- 44m 23s) (8300 8%) 0.5924\n",
      "4m 5s (- 44m 34s) (8400 8%) 0.6733\n",
      "4m 9s (- 44m 47s) (8500 8%) 0.5265\n",
      "4m 13s (- 44m 53s) (8600 8%) 0.6381\n",
      "4m 16s (- 44m 53s) (8700 8%) 0.6039\n",
      "4m 19s (- 44m 48s) (8800 8%) 0.6832\n",
      "4m 22s (- 44m 50s) (8900 8%) 0.6832\n",
      "4m 25s (- 44m 46s) (9000 9%) 0.5765\n",
      "4m 28s (- 44m 44s) (9100 9%) 0.6190\n",
      "4m 31s (- 44m 41s) (9200 9%) 0.6318\n",
      "4m 35s (- 44m 43s) (9300 9%) 0.5866\n",
      "4m 38s (- 44m 42s) (9400 9%) 0.7257\n",
      "4m 41s (- 44m 37s) (9500 9%) 0.6081\n",
      "4m 43s (- 44m 31s) (9600 9%) 0.5495\n",
      "4m 46s (- 44m 26s) (9700 9%) 0.6161\n",
      "4m 48s (- 44m 18s) (9800 9%) 0.4999\n",
      "4m 51s (- 44m 16s) (9900 9%) 0.5842\n",
      "4m 55s (- 44m 17s) (10000 10%) 0.5526\n",
      "4m 58s (- 44m 12s) (10100 10%) 0.5182\n",
      "5m 0s (- 44m 9s) (10200 10%) 0.5436\n",
      "5m 4s (- 44m 8s) (10300 10%) 0.4242\n",
      "5m 6s (- 44m 0s) (10400 10%) 0.4863\n",
      "5m 8s (- 43m 50s) (10500 10%) 0.5226\n",
      "5m 10s (- 43m 37s) (10600 10%) 0.6741\n",
      "5m 12s (- 43m 25s) (10700 10%) 0.4699\n",
      "5m 14s (- 43m 13s) (10800 10%) 0.4937\n",
      "5m 16s (- 43m 3s) (10900 10%) 0.5368\n",
      "5m 18s (- 42m 55s) (11000 11%) 0.5986\n",
      "5m 20s (- 42m 44s) (11100 11%) 0.5091\n",
      "5m 22s (- 42m 34s) (11200 11%) 0.5278\n",
      "5m 25s (- 42m 34s) (11300 11%) 0.6167\n",
      "5m 28s (- 42m 29s) (11400 11%) 0.4065\n",
      "5m 30s (- 42m 26s) (11500 11%) 0.5209\n",
      "5m 33s (- 42m 24s) (11600 11%) 0.5567\n",
      "5m 36s (- 42m 21s) (11700 11%) 0.3920\n",
      "5m 39s (- 42m 18s) (11800 11%) 0.4111\n",
      "5m 42s (- 42m 16s) (11900 11%) 0.3619\n",
      "5m 45s (- 42m 12s) (12000 12%) 0.4395\n",
      "5m 48s (- 42m 13s) (12100 12%) 0.4624\n",
      "5m 51s (- 42m 11s) (12200 12%) 0.5048\n",
      "5m 54s (- 42m 9s) (12300 12%) 0.5048\n",
      "5m 57s (- 42m 7s) (12400 12%) 0.4531\n",
      "6m 0s (- 42m 6s) (12500 12%) 0.5072\n",
      "6m 3s (- 42m 4s) (12600 12%) 0.5333\n",
      "6m 6s (- 41m 58s) (12700 12%) 0.5042\n",
      "6m 8s (- 41m 53s) (12800 12%) 0.4606\n",
      "6m 12s (- 41m 52s) (12900 12%) 0.4359\n",
      "6m 15s (- 41m 50s) (13000 13%) 0.5175\n",
      "6m 17s (- 41m 42s) (13100 13%) 0.4689\n",
      "6m 19s (- 41m 35s) (13200 13%) 0.4367\n",
      "6m 22s (- 41m 35s) (13300 13%) 0.4879\n",
      "6m 25s (- 41m 33s) (13400 13%) 0.4547\n",
      "6m 28s (- 41m 31s) (13500 13%) 0.5113\n",
      "6m 32s (- 41m 32s) (13600 13%) 0.4683\n",
      "6m 35s (- 41m 31s) (13700 13%) 0.4525\n",
      "6m 38s (- 41m 27s) (13800 13%) 0.4095\n",
      "6m 40s (- 41m 23s) (13900 13%) 0.4511\n",
      "6m 44s (- 41m 23s) (14000 14%) 0.4127\n",
      "6m 46s (- 41m 19s) (14100 14%) 0.4198\n",
      "6m 50s (- 41m 17s) (14200 14%) 0.4559\n",
      "6m 53s (- 41m 15s) (14300 14%) 0.4148\n",
      "6m 55s (- 41m 11s) (14400 14%) 0.4795\n",
      "6m 58s (- 41m 9s) (14500 14%) 0.5011\n",
      "7m 2s (- 41m 8s) (14600 14%) 0.4717\n",
      "7m 4s (- 41m 3s) (14700 14%) 0.4698\n",
      "7m 7s (- 41m 3s) (14800 14%) 0.4716\n",
      "7m 10s (- 40m 59s) (14900 14%) 0.4345\n",
      "7m 13s (- 40m 58s) (15000 15%) 0.4789\n",
      "7m 16s (- 40m 55s) (15100 15%) 0.4782\n",
      "7m 19s (- 40m 51s) (15200 15%) 0.3898\n",
      "7m 21s (- 40m 46s) (15300 15%) 0.4335\n",
      "7m 25s (- 40m 45s) (15400 15%) 0.3085\n",
      "7m 27s (- 40m 42s) (15500 15%) 0.5056\n",
      "7m 31s (- 40m 40s) (15600 15%) 0.5073\n",
      "7m 34s (- 40m 37s) (15700 15%) 0.5083\n",
      "7m 37s (- 40m 35s) (15800 15%) 0.3990\n",
      "7m 39s (- 40m 32s) (15900 15%) 0.4299\n",
      "7m 43s (- 40m 31s) (16000 16%) 0.3754\n",
      "7m 45s (- 40m 26s) (16100 16%) 0.4777\n",
      "7m 48s (- 40m 25s) (16200 16%) 0.3933\n",
      "7m 52s (- 40m 24s) (16300 16%) 0.5343\n",
      "7m 55s (- 40m 22s) (16400 16%) 0.4880\n",
      "7m 57s (- 40m 17s) (16500 16%) 0.4002\n",
      "8m 0s (- 40m 14s) (16600 16%) 0.4767\n",
      "8m 4s (- 40m 14s) (16700 16%) 0.4347\n",
      "8m 7s (- 40m 14s) (16800 16%) 0.4340\n",
      "8m 10s (- 40m 13s) (16900 16%) 0.3842\n",
      "8m 13s (- 40m 11s) (17000 17%) 0.3912\n",
      "8m 17s (- 40m 10s) (17100 17%) 0.4430\n",
      "8m 20s (- 40m 7s) (17200 17%) 0.4208\n",
      "8m 23s (- 40m 7s) (17300 17%) 0.4524\n",
      "8m 26s (- 40m 5s) (17400 17%) 0.4071\n",
      "8m 29s (- 40m 0s) (17500 17%) 0.4720\n",
      "8m 32s (- 40m 1s) (17600 17%) 0.4775\n",
      "8m 36s (- 39m 59s) (17700 17%) 0.4675\n",
      "8m 39s (- 39m 57s) (17800 17%) 0.5192\n",
      "8m 41s (- 39m 53s) (17900 17%) 0.4673\n",
      "8m 44s (- 39m 50s) (18000 18%) 0.4963\n",
      "8m 47s (- 39m 49s) (18100 18%) 0.3724\n",
      "8m 51s (- 39m 46s) (18200 18%) 0.4023\n",
      "8m 53s (- 39m 42s) (18300 18%) 0.3963\n",
      "8m 56s (- 39m 39s) (18400 18%) 0.4273\n",
      "8m 59s (- 39m 36s) (18500 18%) 0.3879\n",
      "9m 2s (- 39m 35s) (18600 18%) 0.5347\n",
      "9m 5s (- 39m 32s) (18700 18%) 0.4655\n",
      "9m 8s (- 39m 29s) (18800 18%) 0.4752\n",
      "9m 12s (- 39m 28s) (18900 18%) 0.4224\n",
      "9m 14s (- 39m 24s) (19000 19%) 0.4439\n",
      "9m 17s (- 39m 20s) (19100 19%) 0.3373\n",
      "9m 20s (- 39m 18s) (19200 19%) 0.3838\n",
      "9m 23s (- 39m 15s) (19300 19%) 0.4017\n",
      "9m 26s (- 39m 12s) (19400 19%) 0.3748\n",
      "9m 29s (- 39m 9s) (19500 19%) 0.4323\n",
      "9m 32s (- 39m 7s) (19600 19%) 0.5001\n",
      "9m 35s (- 39m 5s) (19700 19%) 0.3815\n",
      "9m 38s (- 39m 4s) (19800 19%) 0.4469\n",
      "9m 41s (- 39m 0s) (19900 19%) 0.3667\n",
      "9m 44s (- 38m 57s) (20000 20%) 0.4124\n",
      "9m 47s (- 38m 55s) (20100 20%) 0.4037\n",
      "9m 50s (- 38m 52s) (20200 20%) 0.5081\n",
      "9m 53s (- 38m 48s) (20300 20%) 0.4194\n",
      "9m 56s (- 38m 46s) (20400 20%) 0.3686\n",
      "9m 59s (- 38m 43s) (20500 20%) 0.3840\n",
      "10m 2s (- 38m 41s) (20600 20%) 0.4194\n",
      "10m 5s (- 38m 39s) (20700 20%) 0.3445\n",
      "10m 9s (- 38m 39s) (20800 20%) 0.4656\n",
      "10m 12s (- 38m 37s) (20900 20%) 0.3841\n",
      "10m 15s (- 38m 36s) (21000 21%) 0.4627\n",
      "10m 18s (- 38m 32s) (21100 21%) 0.3499\n",
      "10m 21s (- 38m 30s) (21200 21%) 0.3834\n",
      "10m 25s (- 38m 29s) (21300 21%) 0.4874\n",
      "10m 27s (- 38m 25s) (21400 21%) 0.3919\n",
      "10m 30s (- 38m 22s) (21500 21%) 0.4147\n",
      "10m 33s (- 38m 18s) (21600 21%) 0.3394\n",
      "10m 36s (- 38m 17s) (21700 21%) 0.3992\n",
      "10m 39s (- 38m 14s) (21800 21%) 0.4216\n",
      "10m 42s (- 38m 12s) (21900 21%) 0.5123\n",
      "10m 45s (- 38m 9s) (22000 22%) 0.3928\n",
      "10m 48s (- 38m 7s) (22100 22%) 0.5326\n",
      "10m 51s (- 38m 4s) (22200 22%) 0.3834\n",
      "10m 55s (- 38m 2s) (22300 22%) 0.4115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m 58s (- 37m 59s) (22400 22%) 0.4403\n",
      "11m 0s (- 37m 55s) (22500 22%) 0.4507\n",
      "11m 3s (- 37m 52s) (22600 22%) 0.4173\n",
      "11m 6s (- 37m 50s) (22700 22%) 0.3902\n",
      "11m 9s (- 37m 47s) (22800 22%) 0.4085\n",
      "11m 12s (- 37m 44s) (22900 22%) 0.3962\n",
      "11m 15s (- 37m 41s) (23000 23%) 0.4142\n",
      "11m 18s (- 37m 38s) (23100 23%) 0.2663\n",
      "11m 21s (- 37m 36s) (23200 23%) 0.3845\n",
      "11m 24s (- 37m 31s) (23300 23%) 0.3722\n",
      "11m 26s (- 37m 25s) (23400 23%) 0.3629\n",
      "11m 27s (- 37m 19s) (23500 23%) 0.3858\n",
      "11m 29s (- 37m 12s) (23600 23%) 0.5159\n",
      "11m 31s (- 37m 6s) (23700 23%) 0.2931\n",
      "11m 35s (- 37m 6s) (23800 23%) 0.4519\n",
      "11m 39s (- 37m 6s) (23900 23%) 0.3851\n",
      "11m 43s (- 37m 7s) (24000 24%) 0.4735\n",
      "11m 47s (- 37m 9s) (24100 24%) 0.4676\n",
      "11m 51s (- 37m 9s) (24200 24%) 0.3817\n",
      "11m 55s (- 37m 9s) (24300 24%) 0.4600\n",
      "12m 0s (- 37m 11s) (24400 24%) 0.4147\n",
      "12m 4s (- 37m 11s) (24500 24%) 0.4409\n",
      "12m 8s (- 37m 11s) (24600 24%) 0.3900\n",
      "12m 12s (- 37m 11s) (24700 24%) 0.4254\n",
      "12m 16s (- 37m 13s) (24800 24%) 0.4305\n",
      "12m 20s (- 37m 13s) (24900 24%) 0.3512\n",
      "12m 24s (- 37m 13s) (25000 25%) 0.3111\n",
      "12m 28s (- 37m 14s) (25100 25%) 0.4270\n",
      "12m 32s (- 37m 14s) (25200 25%) 0.4281\n",
      "12m 36s (- 37m 13s) (25300 25%) 0.4360\n",
      "12m 40s (- 37m 14s) (25400 25%) 0.4660\n",
      "12m 44s (- 37m 14s) (25500 25%) 0.4458\n",
      "12m 48s (- 37m 14s) (25600 25%) 0.4460\n",
      "12m 53s (- 37m 14s) (25700 25%) 0.4135\n",
      "12m 57s (- 37m 15s) (25800 25%) 0.4163\n",
      "13m 1s (- 37m 14s) (25900 25%) 0.4172\n",
      "13m 5s (- 37m 14s) (26000 26%) 0.4656\n",
      "13m 9s (- 37m 15s) (26100 26%) 0.4218\n",
      "13m 13s (- 37m 14s) (26200 26%) 0.4585\n",
      "13m 17s (- 37m 14s) (26300 26%) 0.3932\n",
      "13m 21s (- 37m 15s) (26400 26%) 0.4372\n",
      "13m 25s (- 37m 15s) (26500 26%) 0.4069\n",
      "13m 29s (- 37m 14s) (26600 26%) 0.4170\n",
      "13m 34s (- 37m 14s) (26700 26%) 0.3488\n",
      "13m 38s (- 37m 14s) (26800 26%) 0.3794\n",
      "13m 42s (- 37m 14s) (26900 26%) 0.3414\n",
      "13m 46s (- 37m 14s) (27000 27%) 0.3672\n",
      "13m 50s (- 37m 13s) (27100 27%) 0.4092\n",
      "13m 54s (- 37m 14s) (27200 27%) 0.3789\n",
      "13m 59s (- 37m 14s) (27300 27%) 0.4622\n",
      "14m 3s (- 37m 13s) (27400 27%) 0.3647\n",
      "14m 7s (- 37m 13s) (27500 27%) 0.3890\n",
      "14m 11s (- 37m 12s) (27600 27%) 0.3738\n",
      "14m 15s (- 37m 12s) (27700 27%) 0.3527\n",
      "14m 19s (- 37m 11s) (27800 27%) 0.3771\n",
      "14m 23s (- 37m 11s) (27900 27%) 0.3623\n",
      "14m 27s (- 37m 10s) (28000 28%) 0.4247\n",
      "14m 31s (- 37m 10s) (28100 28%) 0.4369\n",
      "14m 35s (- 37m 9s) (28200 28%) 0.4603\n",
      "14m 39s (- 37m 8s) (28300 28%) 0.3935\n",
      "14m 43s (- 37m 8s) (28400 28%) 0.4337\n",
      "14m 47s (- 37m 7s) (28500 28%) 0.3535\n",
      "14m 52s (- 37m 7s) (28600 28%) 0.3879\n",
      "14m 56s (- 37m 6s) (28700 28%) 0.3313\n",
      "15m 0s (- 37m 5s) (28800 28%) 0.3715\n",
      "15m 3s (- 37m 3s) (28900 28%) 0.3245\n",
      "15m 7s (- 37m 2s) (29000 28%) 0.4285\n",
      "15m 12s (- 37m 2s) (29100 29%) 0.3639\n",
      "15m 15s (- 37m 0s) (29200 29%) 0.4669\n",
      "15m 20s (- 37m 0s) (29300 29%) 0.3478\n",
      "15m 24s (- 37m 0s) (29400 29%) 0.4524\n",
      "15m 28s (- 36m 59s) (29500 29%) 0.4012\n",
      "15m 33s (- 36m 59s) (29600 29%) 0.4447\n",
      "15m 37s (- 36m 59s) (29700 29%) 0.3520\n",
      "15m 42s (- 36m 59s) (29800 29%) 0.3092\n",
      "15m 46s (- 36m 58s) (29900 29%) 0.4468\n",
      "15m 50s (- 36m 58s) (30000 30%) 0.4562\n",
      "15m 55s (- 36m 57s) (30100 30%) 0.3968\n",
      "15m 59s (- 36m 57s) (30200 30%) 0.4342\n",
      "16m 3s (- 36m 57s) (30300 30%) 0.4221\n",
      "16m 8s (- 36m 56s) (30400 30%) 0.3674\n",
      "16m 12s (- 36m 56s) (30500 30%) 0.4155\n",
      "16m 17s (- 36m 56s) (30600 30%) 0.3801\n",
      "16m 21s (- 36m 56s) (30700 30%) 0.4556\n",
      "16m 26s (- 36m 56s) (30800 30%) 0.3751\n",
      "16m 30s (- 36m 56s) (30900 30%) 0.4184\n",
      "16m 35s (- 36m 55s) (31000 31%) 0.4311\n",
      "16m 39s (- 36m 54s) (31100 31%) 0.4692\n",
      "16m 43s (- 36m 53s) (31200 31%) 0.3629\n",
      "16m 48s (- 36m 54s) (31300 31%) 0.3323\n",
      "16m 53s (- 36m 54s) (31400 31%) 0.3969\n",
      "16m 58s (- 36m 54s) (31500 31%) 0.3623\n",
      "17m 2s (- 36m 53s) (31600 31%) 0.4265\n",
      "17m 6s (- 36m 52s) (31700 31%) 0.4558\n",
      "17m 11s (- 36m 51s) (31800 31%) 0.4347\n",
      "17m 16s (- 36m 51s) (31900 31%) 0.3875\n",
      "17m 20s (- 36m 50s) (32000 32%) 0.3712\n",
      "17m 24s (- 36m 49s) (32100 32%) 0.3998\n",
      "17m 28s (- 36m 48s) (32200 32%) 0.3928\n",
      "17m 33s (- 36m 47s) (32300 32%) 0.4253\n",
      "17m 37s (- 36m 46s) (32400 32%) 0.3352\n",
      "17m 42s (- 36m 46s) (32500 32%) 0.3471\n",
      "17m 46s (- 36m 45s) (32600 32%) 0.4746\n",
      "17m 51s (- 36m 44s) (32700 32%) 0.3460\n",
      "17m 55s (- 36m 44s) (32800 32%) 0.4152\n",
      "18m 0s (- 36m 43s) (32900 32%) 0.3651\n",
      "18m 4s (- 36m 42s) (33000 33%) 0.3657\n",
      "18m 9s (- 36m 41s) (33100 33%) 0.3015\n",
      "18m 13s (- 36m 40s) (33200 33%) 0.3358\n",
      "18m 18s (- 36m 39s) (33300 33%) 0.3806\n",
      "18m 22s (- 36m 39s) (33400 33%) 0.3310\n",
      "18m 27s (- 36m 38s) (33500 33%) 0.3646\n",
      "18m 31s (- 36m 37s) (33600 33%) 0.3778\n",
      "18m 36s (- 36m 36s) (33700 33%) 0.3527\n",
      "18m 41s (- 36m 35s) (33800 33%) 0.4569\n",
      "18m 45s (- 36m 34s) (33900 33%) 0.4153\n",
      "18m 49s (- 36m 33s) (34000 34%) 0.4002\n",
      "18m 54s (- 36m 31s) (34100 34%) 0.4358\n",
      "18m 58s (- 36m 30s) (34200 34%) 0.4668\n",
      "19m 2s (- 36m 28s) (34300 34%) 0.3407\n",
      "19m 7s (- 36m 27s) (34400 34%) 0.4011\n",
      "19m 11s (- 36m 26s) (34500 34%) 0.4105\n",
      "19m 16s (- 36m 25s) (34600 34%) 0.3996\n",
      "19m 20s (- 36m 24s) (34700 34%) 0.4077\n",
      "19m 25s (- 36m 23s) (34800 34%) 0.4169\n",
      "19m 29s (- 36m 22s) (34900 34%) 0.4746\n",
      "19m 34s (- 36m 20s) (35000 35%) 0.4024\n",
      "19m 38s (- 36m 19s) (35100 35%) 0.4084\n",
      "19m 42s (- 36m 17s) (35200 35%) 0.3996\n",
      "19m 47s (- 36m 16s) (35300 35%) 0.3753\n",
      "19m 51s (- 36m 14s) (35400 35%) 0.4110\n",
      "19m 56s (- 36m 13s) (35500 35%) 0.3969\n",
      "20m 0s (- 36m 12s) (35600 35%) 0.4127\n",
      "20m 5s (- 36m 10s) (35700 35%) 0.3826\n",
      "20m 9s (- 36m 9s) (35800 35%) 0.3993\n",
      "20m 14s (- 36m 8s) (35900 35%) 0.3518\n",
      "20m 19s (- 36m 7s) (36000 36%) 0.4144\n",
      "20m 23s (- 36m 5s) (36100 36%) 0.3344\n",
      "20m 27s (- 36m 3s) (36200 36%) 0.4007\n",
      "20m 32s (- 36m 2s) (36300 36%) 0.2945\n",
      "20m 36s (- 36m 0s) (36400 36%) 0.4101\n",
      "20m 41s (- 35m 59s) (36500 36%) 0.3904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6f4afc6c8314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmy_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-081c8e5932e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         loss = train_one_epoch(input_variable, target_variable, encoder,\n\u001b[0;32m---> 20\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-de218b74de65>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "batches = 100000 # In this case, the PyTorch code is using batch_size=1\n",
    "hidden_size = 100\n",
    "\n",
    "singlish_vocab\n",
    "my_encoder = EncoderRNN(len(singlish_vocab), hidden_size)\n",
    "my_decoder = DecoderRNN(hidden_size, len(english_vocab))\n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder = my_encoder.cuda()\n",
    "    my_decoder = my_decoder.cuda()\n",
    "\n",
    "train(my_encoder, my_decoder, batches, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# Before moving on, SAVE THE MODELS!!!\n",
    "with open('encoder_vanilla_100_100K.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_encoder, fout)\n",
    "    \n",
    "with open('decoder_vanilla_100_100K.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_decoder, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6. Getting the Model to Translate\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoder, decoder, input_variable, max_length=MAX_LENGTH):\n",
    "    # The length of the input.\n",
    "    input_length = input_variable.size()[0]\n",
    "    # For each sentence, initilize the hidden states with zeros.\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "    # Initialize the encoder outputs. \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    # Iterate through the input words.\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    # Initialize the decoder with the start symbol <s>.\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]])) \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    # Use the last encoder hidden state as the first decoder's hidden state.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # Keep a list of the decoded words.\n",
    "    decoded_words = []\n",
    "    \n",
    "    # Iterate through the decoder states.\n",
    "    for di in range(max_length):\n",
    "        # Very similar to how the training works.\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == END:\n",
    "            decoded_words.append('</s>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ni)\n",
    "        # Replace the new decoder input for the next state \n",
    "        # with the top guess of this state.\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'teh o'\n",
    "variable_from_sent(sent, singlish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words = decode(my_encoder, my_decoder, variable_from_sent(sent, singlish_vocab))\n",
    "output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[english_vocab[i] for i in output_words[1:output_words.index(1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(kopi_order):\n",
    "    output_words = evaluate(my_encoder, my_decoder, variable_from_sent(kopi_order, singlish_vocab))\n",
    "    output_sentence = [english_vocab[i] for i in output_words[1:output_words.index(1)]]\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('teh o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model (with teacher forcing)\n",
    "====\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "“Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can “pick up” the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch’s autograd gives us, we can randomly choose to use teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio up to use more of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "def train_one_epoch(input_variable, target_variable, encoder, decoder, \n",
    "                    encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to put the variables, decoder and optimizers to train per epoch.\n",
    "    \"\"\"\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "\n",
    "    # (Re-)Initialize the optimizers, clear all gradients. \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize the length of the PyTorch variables.\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # Iterating through each word in the input.\n",
    "    for ei in range(input_length):\n",
    "        # We move forward through each state.\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        # And we save the encoder outputs. \n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input.\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == END_IDX:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 4s (- 71m 9s) (100 0%) 3.0145\n",
      "0m 8s (- 69m 53s) (200 0%) 2.5526\n",
      "0m 12s (- 69m 54s) (300 0%) 2.3090\n",
      "0m 16s (- 69m 37s) (400 0%) 2.3130\n",
      "0m 21s (- 69m 57s) (500 0%) 2.4043\n",
      "0m 25s (- 69m 45s) (600 0%) 2.0384\n",
      "0m 29s (- 70m 6s) (700 0%) 2.0895\n",
      "0m 34s (- 70m 38s) (800 0%) 2.0777\n",
      "0m 38s (- 70m 28s) (900 0%) 2.0000\n",
      "0m 42s (- 70m 31s) (1000 1%) 2.0120\n",
      "0m 47s (- 70m 26s) (1100 1%) 1.9481\n",
      "0m 51s (- 70m 28s) (1200 1%) 1.7242\n",
      "0m 55s (- 69m 57s) (1300 1%) 1.6484\n",
      "0m 59s (- 69m 52s) (1400 1%) 1.7458\n",
      "1m 3s (- 69m 30s) (1500 1%) 1.8199\n",
      "1m 7s (- 69m 33s) (1600 1%) 1.5612\n",
      "1m 12s (- 69m 50s) (1700 1%) 1.6924\n",
      "1m 16s (- 69m 50s) (1800 1%) 1.5385\n",
      "1m 20s (- 69m 41s) (1900 1%) 1.4304\n",
      "1m 25s (- 69m 53s) (2000 2%) 1.4741\n",
      "1m 30s (- 70m 6s) (2100 2%) 1.4806\n",
      "1m 34s (- 70m 9s) (2200 2%) 1.3068\n",
      "1m 39s (- 70m 9s) (2300 2%) 1.3519\n",
      "1m 43s (- 70m 2s) (2400 2%) 1.2785\n",
      "1m 47s (- 69m 55s) (2500 2%) 1.5085\n",
      "1m 51s (- 69m 43s) (2600 2%) 1.1869\n",
      "1m 55s (- 69m 34s) (2700 2%) 1.3052\n",
      "2m 0s (- 69m 30s) (2800 2%) 1.1777\n",
      "2m 4s (- 69m 31s) (2900 2%) 1.3389\n",
      "2m 9s (- 69m 33s) (3000 3%) 1.1533\n",
      "2m 13s (- 69m 31s) (3100 3%) 1.3187\n",
      "2m 17s (- 69m 24s) (3200 3%) 1.0056\n",
      "2m 21s (- 69m 18s) (3300 3%) 1.1916\n",
      "2m 25s (- 68m 54s) (3400 3%) 1.1638\n",
      "2m 29s (- 68m 39s) (3500 3%) 1.1332\n",
      "2m 33s (- 68m 30s) (3600 3%) 1.0626\n",
      "2m 35s (- 67m 39s) (3700 3%) 1.0748\n",
      "2m 38s (- 67m 0s) (3800 3%) 1.0585\n",
      "2m 43s (- 66m 57s) (3900 3%) 1.0573\n",
      "2m 47s (- 66m 56s) (4000 4%) 0.8928\n",
      "2m 51s (- 66m 58s) (4100 4%) 0.9947\n",
      "2m 56s (- 66m 57s) (4200 4%) 0.8721\n",
      "3m 0s (- 66m 56s) (4300 4%) 0.9378\n",
      "3m 4s (- 66m 58s) (4400 4%) 0.8377\n",
      "3m 9s (- 66m 58s) (4500 4%) 1.0158\n",
      "3m 13s (- 66m 55s) (4600 4%) 0.9408\n",
      "3m 16s (- 66m 34s) (4700 4%) 0.9472\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "batches = 100000 # In this case, the PyTorch code is using batch_size=1\n",
    "hidden_size = 100\n",
    "\n",
    "singlish_vocab\n",
    "my_encoder = EncoderRNN(len(singlish_vocab), hidden_size)\n",
    "my_decoder = DecoderRNN(hidden_size, len(english_vocab))\n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder = my_encoder.cuda()\n",
    "    my_decoder = my_decoder.cuda()\n",
    "\n",
    "train(my_encoder, my_decoder, batches, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before moving on, SAVE THE MODELS!!!\n",
    "with open('encoder_vanilla_100_100K_teachforce.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_encoder, fout)\n",
    "    \n",
    "with open('decoder_vanilla_100_100K_teachforce.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_decoder, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea with evaporated milk and sugar'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea with condensed milk and more sugar'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh ga dai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea with evaporated milk and more sugar'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh c ga dai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea with evaporated milk and sugar'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh c ga dai peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea with lesser sugar'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh o siew dai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heaviest , purest version of tea with no water added at all to the initial brew'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh tiloh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced version of tea with sugar'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh tiloh peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced milo'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('tak kiu peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soya bean milk mixed with grass jelly'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('michael jackson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ice milo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('michael jackson peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced tea with sugar'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh siew dai peng')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
