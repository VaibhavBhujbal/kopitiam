{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython candies...\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq with PyTorch\n",
    "====\n",
    "\n",
    "Sequence-to-Sequence (Seq2Seq) learning is a useful class of neural network model to map sequential input into an output sequence. It has been shown to work well on various task, from machine translation to interpreting Python without an interpreter. {{citations-needed}}\n",
    "\n",
    "This notebook is a hands-on session to write an encoder-decoder Seq2Seq network using PyTorch for [DataScience SG meetup](https://www.meetup.com/DataScience-SG-Singapore/events/246541733/). \n",
    "\n",
    "\n",
    "It would be great if you have at least worked through the [\"Deep Learning in 60 minutes\" PyTorch tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) before continuing the rest of the notebook.\n",
    "\n",
    "\n",
    "Acknowledgements\n",
    "----\n",
    "The materials are largely based on the \n",
    "\n",
    " - [intermediate PyTorch tutorials by Sean Robertson](http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) and \n",
    " - [Luong et al. tutorial on neural machine translation in ACL16](https://sites.google.com/site/acl16nmt/home).\n",
    "\n",
    "The dataset used in this exercise is hosted on https://www.kaggle.com/alvations/sg-kopi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopi Problems\n",
    "====\n",
    "\n",
    "In this hands-on session, we want to **train a neural network to translate from Singlish Kopi orders to English?**\n",
    "\n",
    "\n",
    "**\"Singlish\" -> English**\n",
    "\n",
    "```\n",
    "\"Kopi\" -> Coffee with condensed milk\n",
    "\"Kopi O\" -> Coffee without milk or sugar\n",
    "\"Kopi dinosaur gau siew dai peng\" -> ???\n",
    "```\n",
    "\n",
    "(Image Source: http://www.straitstimes.com/lifestyle/food/get-your-kopi-kick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://static.straitstimes.com.sg/sites/default/files/160522_kopi.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://static.straitstimes.com.sg/sites/default/files/160522_kopi.jpg\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriously?\n",
    "----\n",
    "\n",
    "Yes, we'll be translating Singlish Kopi orders to English using the [sequence-to-sequence network](https://arxiv.org/abs/1409.3215) {{citations-needed}}. \n",
    "\n",
    "But first...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Munging\n",
    "====\n",
    "\n",
    "Before any machine/deep learning, we have to get some data and \"hammer\" it until we get it into the shape we want.\n",
    "\n",
    "> *Data scientists spend 60% of their time on cleaning and organizing data. Collecting data sets comes second at 19% of their time, meaning data scientists spend around 80% of their time on preparing and managing data for analysis.*\n",
    "\n",
    "> (Source: [Gil Press](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#3e4dc0416f63) Forbes article)\n",
    "\n",
    "**Step 1:** Take the data from somewhere, in this case: http://kaggle.com/alvations/sg-kopi.\n",
    "\n",
    "**Step 2:** Import your favorite dataframe and text processing library.\n",
    "\n",
    "**Step 3:** Munge the data till desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Local Terms</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kopi O</td>\n",
       "      <td>Black Coffee with Sugar</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kopi</td>\n",
       "      <td>Black Coffee with Condensed Milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kopi C</td>\n",
       "      <td>Black Coffee with Evaporated Milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kopi Kosong</td>\n",
       "      <td>Black Coffee without sugar or milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kopi Gah Dai</td>\n",
       "      <td>Black Coffee with extra condensed milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Local Terms                                 Meaning  \\\n",
       "0        Kopi O                 Black Coffee with Sugar   \n",
       "1          Kopi        Black Coffee with Condensed Milk   \n",
       "2        Kopi C       Black Coffee with Evaporated Milk   \n",
       "3   Kopi Kosong      Black Coffee without sugar or milk   \n",
       "4  Kopi Gah Dai  Black Coffee with extra condensed milk   \n",
       "\n",
       "                                              Source  \n",
       "0  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "1  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "2  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "3  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "4  https://daneshd.com/2010/02/28/a-rough-guide-t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Reads the tab-delimited data using Pandas.\n",
    "kopitiam = pd.read_csv('coffee-culture-sg.tsv', sep='\\t')\n",
    "kopitiam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence: ['<s>', 'kopi', 'o', '</s>']\n",
      "First English sentence: ['<s>', 'black', 'coffee', 'with', 'sugar', '</s>']\n"
     ]
    }
   ],
   "source": [
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "singlish_sents = [START] + kopitiam['Local Terms'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "english_sents = [START] + kopitiam['Meaning'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First Singlish sentence:', singlish_sents[0])\n",
    "print('First English sentence:', english_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Singlish words:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, 'kopi'), (4, 'o'), (5, 'c'), (6, 'kosong'), (7, 'dai'), (8, 'gah'), (9, 'siew')]\n",
      "\n",
      "First 10 English words:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, 'black'), (4, 'coffee'), (5, 'sugar'), (6, 'with'), (7, 'condensed'), (8, 'milk'), (9, 'evaporated')]\n"
     ]
    }
   ],
   "source": [
    "# Let's convert the individual words into some sort of unique index \n",
    "# and use the unique to represent the words. \n",
    "## Cut-away: Integers = 1-2 bytes vs UTF-8 Strings = no. of chars * 1-2 bytes. @_@\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "singlish_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "singlish_vocab.add_documents(singlish_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Singlish words:\\n', sorted(singlish_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence: [0, 3, 4, 1]\n",
      "First English sentence: [0, 3, 4, 6, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "# Now, convert all the sentences into list of the indices \n",
    "print('First Singlish sentence:', singlish_vocab.doc2idx(singlish_sents[0]) )\n",
    "print('First English sentence:', english_vocab.doc2idx(english_sents[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 22, 11, 9, 7, 12, 1]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets create a function to convert new sentences into the indexed forms.\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END])\n",
    "\n",
    "new_kopi = \"Kopi dinosaur gau siew dai peng\"\n",
    "vectorize_sent(new_kopi, singlish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0\n",
       "    3\n",
       "   22\n",
       "   11\n",
       "    9\n",
       "    7\n",
       "   12\n",
       "    1\n",
       "[torch.LongTensor of size 8x1]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the last step of data hammering, we need to clobber \n",
    "# the vectorized sentence into PyTorch Variable. \n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "new_kopi = \"Kopi dinosaur gau siew dai peng\"\n",
    "variable_from_sent(new_kopi, singlish_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the sentence length.\n",
    "variable_from_sent(new_kopi, singlish_vocab).size()[0] # Includes START and END symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the whole training corpus.\n",
    "singlish_tensors = kopitiam['Local Terms'].apply(lambda s: variable_from_sent(s, singlish_vocab))\n",
    "english_tensors = kopitiam['Meaning'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "\n",
    "sent_pairs = list(zip(singlish_tensors, english_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "====\n",
    "\n",
    "A Recurrent Neural Network (RNN), is a network that operates on a sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "> *The general idea is to make **two recurrent neural network transform from one sequence to another**. An encoder network condenses an input sequence into a vector and a decoder netwrok unfolds the vector into a new sequence.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "====\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
    "\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/encoder-network.png\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "====\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string `<s>` token, and the first hidden state is the context vector (the encoder’s last hidden state).\n",
    "\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/decoder-network.png\" align='left'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of output (i.e. no. of words in output vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Set the output layer to output a specific symbol \n",
    "        # from the output vocabulary\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # Transform the embedded output with a relu function. \n",
    "        output = F.relu(output)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Take the updated output and find the most appropriate\n",
    "        # output symbol. \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "====\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "“Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can “pick up” the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch’s autograd gives us, we can randomly choose to use teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio up to use more of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "def train_one_epoch(input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to put the variables, decoder and optimizers to train per epoch.\n",
    "    \"\"\"\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "\n",
    "    # (Re-)Initialize the optimizers, clear all gradients. \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize the length of the PyTorch variables.\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == END_IDX:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    training_pairs = [random.choice(sent_pairs) for i in range(n_iters)]\n",
    "    \n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train_one_epoch(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 7s (- 38m 29s) (100 0%) 2.6505\n",
      "0m 15s (- 37m 53s) (200 0%) 2.2769\n",
      "0m 23s (- 38m 20s) (300 1%) 2.1790\n",
      "0m 31s (- 38m 19s) (400 1%) 1.9496\n",
      "0m 38s (- 38m 5s) (500 1%) 1.9081\n",
      "0m 46s (- 38m 12s) (600 2%) 1.8574\n",
      "0m 54s (- 38m 0s) (700 2%) 1.9087\n",
      "1m 2s (- 37m 57s) (800 2%) 1.7099\n",
      "1m 10s (- 37m 51s) (900 3%) 1.4763\n",
      "1m 18s (- 37m 48s) (1000 3%) 1.4021\n",
      "1m 26s (- 37m 46s) (1100 3%) 1.2450\n",
      "1m 34s (- 37m 38s) (1200 4%) 1.2951\n",
      "1m 41s (- 37m 30s) (1300 4%) 1.1240\n",
      "1m 49s (- 37m 16s) (1400 4%) 1.0743\n",
      "1m 57s (- 37m 21s) (1500 5%) 1.1606\n",
      "2m 6s (- 37m 18s) (1600 5%) 1.1041\n",
      "2m 14s (- 37m 13s) (1700 5%) 0.8860\n",
      "2m 22s (- 37m 11s) (1800 6%) 1.0419\n",
      "2m 30s (- 37m 11s) (1900 6%) 1.0665\n",
      "2m 38s (- 37m 3s) (2000 6%) 1.1248\n",
      "2m 47s (- 37m 1s) (2100 7%) 0.7572\n",
      "2m 55s (- 36m 51s) (2200 7%) 0.7403\n",
      "3m 2s (- 36m 41s) (2300 7%) 1.1159\n",
      "3m 10s (- 36m 34s) (2400 8%) 0.9563\n",
      "3m 20s (- 36m 40s) (2500 8%) 0.6792\n",
      "3m 27s (- 36m 29s) (2600 8%) 0.8766\n",
      "3m 36s (- 36m 25s) (2700 9%) 0.6764\n",
      "3m 44s (- 36m 18s) (2800 9%) 0.8449\n",
      "3m 52s (- 36m 11s) (2900 9%) 0.7018\n",
      "4m 0s (- 36m 7s) (3000 10%) 0.6213\n",
      "4m 9s (- 36m 4s) (3100 10%) 0.8641\n",
      "4m 17s (- 35m 58s) (3200 10%) 0.7220\n",
      "4m 25s (- 35m 51s) (3300 11%) 0.6746\n",
      "4m 34s (- 35m 46s) (3400 11%) 0.6637\n",
      "4m 42s (- 35m 38s) (3500 11%) 0.4758\n",
      "4m 51s (- 35m 34s) (3600 12%) 0.5750\n",
      "4m 59s (- 35m 28s) (3700 12%) 0.5813\n",
      "5m 7s (- 35m 21s) (3800 12%) 0.4952\n",
      "5m 15s (- 35m 14s) (3900 13%) 0.5264\n",
      "5m 23s (- 35m 4s) (4000 13%) 0.6303\n",
      "5m 32s (- 34m 58s) (4100 13%) 0.5573\n",
      "5m 40s (- 34m 53s) (4200 14%) 0.6489\n",
      "5m 49s (- 34m 47s) (4300 14%) 0.7225\n",
      "5m 57s (- 34m 39s) (4400 14%) 0.5280\n",
      "6m 6s (- 34m 34s) (4500 15%) 0.7036\n",
      "6m 14s (- 34m 27s) (4600 15%) 0.5621\n",
      "6m 22s (- 34m 19s) (4700 15%) 0.5325\n",
      "6m 31s (- 34m 15s) (4800 16%) 0.6964\n",
      "6m 39s (- 34m 7s) (4900 16%) 0.5252\n",
      "6m 48s (- 34m 2s) (5000 16%) 0.5132\n",
      "6m 56s (- 33m 55s) (5100 17%) 0.6519\n",
      "7m 5s (- 33m 48s) (5200 17%) 0.5055\n",
      "7m 13s (- 33m 40s) (5300 17%) 0.5056\n",
      "7m 21s (- 33m 32s) (5400 18%) 0.5882\n",
      "7m 30s (- 33m 28s) (5500 18%) 0.5484\n",
      "7m 40s (- 33m 25s) (5600 18%) 0.7014\n",
      "7m 50s (- 33m 23s) (5700 19%) 0.6337\n",
      "8m 2s (- 33m 32s) (5800 19%) 0.6700\n",
      "8m 13s (- 33m 37s) (5900 19%) 0.5057\n",
      "8m 22s (- 33m 30s) (6000 20%) 0.5579\n",
      "8m 31s (- 33m 22s) (6100 20%) 0.5713\n",
      "8m 40s (- 33m 16s) (6200 20%) 0.4940\n",
      "8m 48s (- 33m 8s) (6300 21%) 0.6417\n",
      "8m 57s (- 33m 0s) (6400 21%) 0.6505\n",
      "9m 5s (- 32m 53s) (6500 21%) 0.5572\n",
      "9m 14s (- 32m 46s) (6600 22%) 0.4252\n",
      "9m 23s (- 32m 40s) (6700 22%) 0.5236\n",
      "9m 34s (- 32m 40s) (6800 22%) 0.4984\n",
      "9m 44s (- 32m 36s) (6900 23%) 0.4613\n",
      "9m 56s (- 32m 38s) (7000 23%) 0.3683\n",
      "10m 4s (- 32m 29s) (7100 23%) 0.4286\n",
      "10m 13s (- 32m 22s) (7200 24%) 0.4432\n",
      "10m 21s (- 32m 13s) (7300 24%) 0.4500\n",
      "10m 32s (- 32m 10s) (7400 24%) 0.3748\n",
      "10m 41s (- 32m 3s) (7500 25%) 0.4225\n",
      "10m 49s (- 31m 54s) (7600 25%) 0.5446\n",
      "10m 58s (- 31m 46s) (7700 25%) 0.4114\n",
      "11m 7s (- 31m 38s) (7800 26%) 0.5625\n",
      "11m 15s (- 31m 28s) (7900 26%) 0.5893\n",
      "11m 23s (- 31m 20s) (8000 26%) 0.5082\n",
      "11m 32s (- 31m 11s) (8100 27%) 0.3563\n",
      "11m 40s (- 31m 3s) (8200 27%) 0.4085\n",
      "11m 49s (- 30m 55s) (8300 27%) 0.6828\n",
      "11m 58s (- 30m 47s) (8400 28%) 0.5254\n",
      "12m 7s (- 30m 40s) (8500 28%) 0.4047\n",
      "12m 19s (- 30m 41s) (8600 28%) 0.4673\n",
      "12m 30s (- 30m 38s) (8700 28%) 0.4509\n",
      "12m 40s (- 30m 31s) (8800 29%) 0.5300\n",
      "12m 48s (- 30m 22s) (8900 29%) 0.5725\n",
      "12m 57s (- 30m 15s) (9000 30%) 0.3482\n",
      "13m 9s (- 30m 12s) (9100 30%) 0.6056\n",
      "13m 17s (- 30m 3s) (9200 30%) 0.4164\n",
      "13m 26s (- 29m 54s) (9300 31%) 0.5545\n",
      "13m 34s (- 29m 45s) (9400 31%) 0.5632\n",
      "13m 44s (- 29m 39s) (9500 31%) 0.4340\n",
      "13m 55s (- 29m 34s) (9600 32%) 0.3607\n",
      "14m 3s (- 29m 24s) (9700 32%) 0.5092\n",
      "14m 12s (- 29m 17s) (9800 32%) 0.5012\n",
      "14m 22s (- 29m 11s) (9900 33%) 0.5818\n",
      "14m 33s (- 29m 6s) (10000 33%) 0.4047\n",
      "14m 43s (- 29m 0s) (10100 33%) 0.3982\n",
      "14m 52s (- 28m 52s) (10200 34%) 0.4048\n",
      "15m 1s (- 28m 44s) (10300 34%) 0.4622\n",
      "15m 11s (- 28m 38s) (10400 34%) 0.5005\n",
      "15m 21s (- 28m 32s) (10500 35%) 0.3557\n",
      "15m 33s (- 28m 29s) (10600 35%) 0.4476\n",
      "15m 43s (- 28m 21s) (10700 35%) 0.5011\n",
      "15m 51s (- 28m 11s) (10800 36%) 0.4386\n",
      "16m 1s (- 28m 4s) (10900 36%) 0.4214\n",
      "16m 12s (- 27m 59s) (11000 36%) 0.4206\n",
      "16m 22s (- 27m 53s) (11100 37%) 0.4285\n",
      "16m 32s (- 27m 45s) (11200 37%) 0.4181\n",
      "16m 42s (- 27m 39s) (11300 37%) 0.5011\n",
      "16m 51s (- 27m 30s) (11400 38%) 0.5396\n",
      "17m 0s (- 27m 21s) (11500 38%) 0.3075\n",
      "17m 11s (- 27m 15s) (11600 38%) 0.5481\n",
      "17m 22s (- 27m 10s) (11700 39%) 0.3826\n",
      "17m 31s (- 27m 2s) (11800 39%) 0.3191\n",
      "17m 41s (- 26m 55s) (11900 39%) 0.4854\n",
      "17m 51s (- 26m 47s) (12000 40%) 0.3848\n",
      "18m 0s (- 26m 39s) (12100 40%) 0.3783\n",
      "18m 9s (- 26m 30s) (12200 40%) 0.3762\n",
      "18m 18s (- 26m 20s) (12300 41%) 0.4592\n",
      "18m 26s (- 26m 11s) (12400 41%) 0.4654\n",
      "18m 35s (- 26m 1s) (12500 41%) 0.3397\n",
      "18m 43s (- 25m 51s) (12600 42%) 0.4466\n",
      "18m 52s (- 25m 42s) (12700 42%) 0.4107\n",
      "19m 0s (- 25m 32s) (12800 42%) 0.3850\n",
      "19m 9s (- 25m 24s) (12900 43%) 0.3575\n",
      "19m 18s (- 25m 14s) (13000 43%) 0.3640\n",
      "19m 26s (- 25m 5s) (13100 43%) 0.4519\n",
      "19m 35s (- 24m 55s) (13200 44%) 0.4365\n",
      "19m 43s (- 24m 46s) (13300 44%) 0.4095\n",
      "19m 52s (- 24m 37s) (13400 44%) 0.4955\n",
      "20m 1s (- 24m 27s) (13500 45%) 0.4671\n",
      "20m 9s (- 24m 18s) (13600 45%) 0.4100\n",
      "20m 17s (- 24m 9s) (13700 45%) 0.5250\n",
      "20m 26s (- 23m 59s) (13800 46%) 0.3689\n",
      "20m 34s (- 23m 50s) (13900 46%) 0.2829\n",
      "20m 43s (- 23m 40s) (14000 46%) 0.4839\n",
      "20m 51s (- 23m 31s) (14100 47%) 0.4066\n",
      "21m 0s (- 23m 22s) (14200 47%) 0.3604\n",
      "21m 8s (- 23m 12s) (14300 47%) 0.5964\n",
      "21m 16s (- 23m 2s) (14400 48%) 0.4270\n",
      "21m 26s (- 22m 55s) (14500 48%) 0.3294\n",
      "21m 35s (- 22m 47s) (14600 48%) 0.3593\n",
      "21m 44s (- 22m 37s) (14700 49%) 0.4447\n",
      "21m 52s (- 22m 28s) (14800 49%) 0.5918\n",
      "22m 1s (- 22m 19s) (14900 49%) 0.4410\n",
      "22m 9s (- 22m 9s) (15000 50%) 0.4379\n",
      "22m 18s (- 22m 0s) (15100 50%) 0.5390\n",
      "22m 28s (- 21m 53s) (15200 50%) 0.4554\n",
      "22m 39s (- 21m 45s) (15300 51%) 0.4938\n",
      "22m 49s (- 21m 38s) (15400 51%) 0.5629\n",
      "22m 58s (- 21m 29s) (15500 51%) 0.4721\n",
      "23m 6s (- 21m 20s) (15600 52%) 0.4221\n",
      "23m 15s (- 21m 10s) (15700 52%) 0.4270\n",
      "23m 23s (- 21m 1s) (15800 52%) 0.4444\n",
      "23m 32s (- 20m 52s) (15900 53%) 0.3661\n",
      "23m 45s (- 20m 47s) (16000 53%) 0.5230\n",
      "24m 2s (- 20m 45s) (16100 53%) 0.3843\n",
      "24m 12s (- 20m 37s) (16200 54%) 0.3456\n",
      "24m 25s (- 20m 31s) (16300 54%) 0.3189\n",
      "24m 36s (- 20m 24s) (16400 54%) 0.4507\n",
      "24m 49s (- 20m 18s) (16500 55%) 0.5084\n",
      "25m 0s (- 20m 11s) (16600 55%) 0.4010\n",
      "25m 9s (- 20m 2s) (16700 55%) 0.4348\n",
      "25m 19s (- 19m 54s) (16800 56%) 0.3831\n",
      "25m 28s (- 19m 44s) (16900 56%) 0.3887\n",
      "25m 38s (- 19m 36s) (17000 56%) 0.3094\n",
      "25m 48s (- 19m 28s) (17100 56%) 0.4073\n",
      "25m 57s (- 19m 18s) (17200 57%) 0.3902\n",
      "26m 5s (- 19m 9s) (17300 57%) 0.4840\n",
      "26m 15s (- 19m 0s) (17400 57%) 0.3527\n",
      "26m 22s (- 18m 50s) (17500 58%) 0.5086\n",
      "26m 32s (- 18m 41s) (17600 58%) 0.5829\n",
      "26m 41s (- 18m 32s) (17700 59%) 0.3601\n",
      "26m 50s (- 18m 23s) (17800 59%) 0.5282\n",
      "26m 57s (- 18m 13s) (17900 59%) 0.5353\n",
      "27m 4s (- 18m 3s) (18000 60%) 0.3637\n",
      "27m 12s (- 17m 53s) (18100 60%) 0.3424\n",
      "27m 20s (- 17m 43s) (18200 60%) 0.4564\n",
      "27m 28s (- 17m 33s) (18300 61%) 0.4389\n",
      "27m 36s (- 17m 24s) (18400 61%) 0.3099\n",
      "27m 43s (- 17m 13s) (18500 61%) 0.4473\n",
      "27m 52s (- 17m 4s) (18600 62%) 0.4164\n",
      "28m 4s (- 16m 57s) (18700 62%) 0.4265\n",
      "28m 16s (- 16m 50s) (18800 62%) 0.2836\n",
      "28m 24s (- 16m 40s) (18900 63%) 0.3399\n",
      "28m 34s (- 16m 32s) (19000 63%) 0.3902\n",
      "28m 44s (- 16m 24s) (19100 63%) 0.3189\n",
      "28m 52s (- 16m 14s) (19200 64%) 0.3954\n",
      "29m 0s (- 16m 4s) (19300 64%) 0.4283\n",
      "29m 10s (- 15m 56s) (19400 64%) 0.4602\n",
      "29m 18s (- 15m 46s) (19500 65%) 0.4040\n",
      "29m 25s (- 15m 36s) (19600 65%) 0.4531\n",
      "29m 32s (- 15m 26s) (19700 65%) 0.4555\n",
      "29m 39s (- 15m 16s) (19800 66%) 0.3198\n",
      "29m 46s (- 15m 6s) (19900 66%) 0.4141\n",
      "29m 54s (- 14m 57s) (20000 66%) 0.4059\n",
      "30m 2s (- 14m 47s) (20100 67%) 0.3662\n",
      "30m 10s (- 14m 38s) (20200 67%) 0.4745\n",
      "30m 17s (- 14m 28s) (20300 67%) 0.4092\n",
      "30m 25s (- 14m 18s) (20400 68%) 0.4239\n",
      "30m 33s (- 14m 9s) (20500 68%) 0.4314\n",
      "30m 41s (- 14m 0s) (20600 68%) 0.3966\n",
      "30m 49s (- 13m 51s) (20700 69%) 0.4552\n",
      "30m 57s (- 13m 41s) (20800 69%) 0.2511\n",
      "31m 4s (- 13m 31s) (20900 69%) 0.4145\n",
      "31m 12s (- 13m 22s) (21000 70%) 0.3144\n",
      "31m 20s (- 13m 13s) (21100 70%) 0.3130\n",
      "31m 30s (- 13m 4s) (21200 70%) 0.3608\n",
      "31m 39s (- 12m 56s) (21300 71%) 0.4450\n",
      "31m 48s (- 12m 46s) (21400 71%) 0.4008\n",
      "31m 56s (- 12m 37s) (21500 71%) 0.4021\n",
      "32m 3s (- 12m 27s) (21600 72%) 0.3350\n",
      "32m 10s (- 12m 18s) (21700 72%) 0.3555\n",
      "32m 17s (- 12m 8s) (21800 72%) 0.3187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32m 24s (- 11m 59s) (21900 73%) 0.4618\n",
      "32m 31s (- 11m 49s) (22000 73%) 0.3104\n",
      "32m 38s (- 11m 40s) (22100 73%) 0.4553\n",
      "32m 46s (- 11m 31s) (22200 74%) 0.4734\n",
      "32m 53s (- 11m 21s) (22300 74%) 0.4470\n",
      "33m 2s (- 11m 12s) (22400 74%) 0.2663\n",
      "33m 16s (- 11m 5s) (22500 75%) 0.3754\n",
      "33m 32s (- 10m 58s) (22600 75%) 0.4359\n",
      "33m 43s (- 10m 50s) (22700 75%) 0.3947\n",
      "33m 51s (- 10m 41s) (22800 76%) 0.3269\n",
      "33m 59s (- 10m 32s) (22900 76%) 0.3110\n",
      "34m 6s (- 10m 22s) (23000 76%) 0.4278\n",
      "34m 13s (- 10m 13s) (23100 77%) 0.3666\n",
      "34m 21s (- 10m 4s) (23200 77%) 0.4850\n",
      "34m 29s (- 9m 55s) (23300 77%) 0.3342\n",
      "34m 37s (- 9m 46s) (23400 78%) 0.4602\n",
      "34m 45s (- 9m 36s) (23500 78%) 0.4955\n",
      "34m 53s (- 9m 27s) (23600 78%) 0.3511\n",
      "35m 3s (- 9m 19s) (23700 79%) 0.3852\n",
      "35m 14s (- 9m 10s) (23800 79%) 0.4021\n",
      "35m 23s (- 9m 2s) (23900 79%) 0.4050\n",
      "35m 31s (- 8m 52s) (24000 80%) 0.3765\n",
      "35m 39s (- 8m 43s) (24100 80%) 0.4599\n",
      "35m 47s (- 8m 34s) (24200 80%) 0.3084\n",
      "35m 55s (- 8m 25s) (24300 81%) 0.4676\n",
      "36m 7s (- 8m 17s) (24400 81%) 0.3272\n",
      "36m 17s (- 8m 8s) (24500 81%) 0.3574\n",
      "36m 25s (- 7m 59s) (24600 82%) 0.3405\n",
      "36m 32s (- 7m 50s) (24700 82%) 0.3062\n",
      "36m 40s (- 7m 41s) (24800 82%) 0.3301\n",
      "36m 48s (- 7m 32s) (24900 83%) 0.3906\n",
      "36m 57s (- 7m 23s) (25000 83%) 0.3179\n",
      "37m 4s (- 7m 14s) (25100 83%) 0.4158\n",
      "37m 12s (- 7m 5s) (25200 84%) 0.4155\n",
      "37m 20s (- 6m 56s) (25300 84%) 0.3368\n",
      "37m 27s (- 6m 47s) (25400 84%) 0.3863\n",
      "37m 36s (- 6m 38s) (25500 85%) 0.4524\n",
      "37m 44s (- 6m 29s) (25600 85%) 0.3925\n",
      "37m 53s (- 6m 20s) (25700 85%) 0.3542\n",
      "38m 0s (- 6m 11s) (25800 86%) 0.4079\n",
      "38m 8s (- 6m 2s) (25900 86%) 0.3243\n",
      "38m 15s (- 5m 53s) (26000 86%) 0.4532\n",
      "38m 23s (- 5m 44s) (26100 87%) 0.3950\n",
      "38m 30s (- 5m 35s) (26200 87%) 0.3261\n",
      "38m 37s (- 5m 26s) (26300 87%) 0.4695\n",
      "38m 45s (- 5m 17s) (26400 88%) 0.4508\n",
      "38m 55s (- 5m 8s) (26500 88%) 0.4292\n",
      "39m 3s (- 4m 59s) (26600 88%) 0.4627\n",
      "39m 10s (- 4m 50s) (26700 89%) 0.4552\n",
      "39m 18s (- 4m 41s) (26800 89%) 0.4566\n",
      "39m 29s (- 4m 33s) (26900 89%) 0.3753\n",
      "39m 37s (- 4m 24s) (27000 90%) 0.2557\n",
      "39m 46s (- 4m 15s) (27100 90%) 0.4822\n",
      "39m 54s (- 4m 6s) (27200 90%) 0.3760\n",
      "40m 2s (- 3m 57s) (27300 91%) 0.3832\n",
      "40m 10s (- 3m 48s) (27400 91%) 0.3591\n",
      "40m 18s (- 3m 39s) (27500 91%) 0.3302\n",
      "40m 25s (- 3m 30s) (27600 92%) 0.3975\n",
      "40m 33s (- 3m 22s) (27700 92%) 0.4174\n",
      "40m 40s (- 3m 13s) (27800 92%) 0.4623\n",
      "40m 48s (- 3m 4s) (27900 93%) 0.3377\n",
      "40m 57s (- 2m 55s) (28000 93%) 0.3825\n",
      "41m 5s (- 2m 46s) (28100 93%) 0.3919\n",
      "41m 12s (- 2m 37s) (28200 94%) 0.3475\n",
      "41m 19s (- 2m 28s) (28300 94%) 0.3521\n",
      "41m 26s (- 2m 20s) (28400 94%) 0.2991\n",
      "41m 34s (- 2m 11s) (28500 95%) 0.3818\n",
      "41m 41s (- 2m 2s) (28600 95%) 0.4127\n",
      "41m 50s (- 1m 53s) (28700 95%) 0.4021\n",
      "41m 58s (- 1m 44s) (28800 96%) 0.3836\n",
      "42m 7s (- 1m 36s) (28900 96%) 0.4532\n",
      "42m 16s (- 1m 27s) (29000 96%) 0.4345\n",
      "42m 23s (- 1m 18s) (29100 97%) 0.3599\n",
      "42m 30s (- 1m 9s) (29200 97%) 0.3804\n",
      "42m 38s (- 1m 1s) (29300 97%) 0.4226\n",
      "42m 45s (- 0m 52s) (29400 98%) 0.4613\n",
      "42m 54s (- 0m 43s) (29500 98%) 0.3717\n",
      "43m 2s (- 0m 34s) (29600 98%) 0.4900\n",
      "43m 9s (- 0m 26s) (29700 99%) 0.3623\n",
      "43m 17s (- 0m 17s) (29800 99%) 0.3847\n",
      "43m 26s (- 0m 8s) (29900 99%) 0.4052\n",
      "43m 34s (- 0m 0s) (30000 100%) 0.3539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95b4777780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95b476f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 500\n",
    "\n",
    "singlish_vocab\n",
    "encoder1 = EncoderRNN(len(singlish_vocab), hidden_size)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, len(english_vocab))\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "train(encoder1, attn_decoder1, 30000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "output_sentence = ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(sent_pairs)\n",
    "        print('>', [singlish_vocab[i] for i in pair[0][0]])\n",
    "        print('=', [english_gocab[i] for i in pair[1][0]])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = [english_vocab[i] for i in output_words[1:output_words.index(1)]]\n",
    "        print('<', output_sentence)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_variable, max_length=MAX_LENGTH):\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    \n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == END:\n",
    "            decoded_words.append('</s>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ni)\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(kopi_order):\n",
    "    output_words = evaluate(encoder1, attn_decoder1, variable_from_sent(kopi_order, singlish_vocab))\n",
    "    output_sentence = [english_vocab[i] for i in output_words[1:output_words.index(1)]]\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tea with condensed milk'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tea without milk but with sugar'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tea with evaporated milk and sugar instead of condensed milk'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea more condensed'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh ga dai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot tea with evaporated milk and more sugar'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh c ga dai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced tea with evaporated milk and sugar'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh c ga dai peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tea with less sugar'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh o siew dai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heaviest , purest version of tea with no water added at all to the initial brew'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh tiloh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced version of tea'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh tiloh peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced milo'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('tak kiu peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soya bean milk mixed with grass jelly'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('michael jackson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced milo'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('michael jackson peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced tea with condensed'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('teh siew dai peng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder_vanilla_500.pkl', 'wb') as fout:\n",
    "    pickle.dump(encoder1, fout)\n",
    "    \n",
    "with open('decoder_vanilla_500.pkl', 'wb') as fout:\n",
    "    pickle.dump(attn_decoder1, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
